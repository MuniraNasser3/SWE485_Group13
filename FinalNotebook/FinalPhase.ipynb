{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab13cddc",
   "metadata": {},
   "source": [
    "#Final Jupytor Notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cd315",
   "metadata": {},
   "source": [
    "#Phase1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd5af3c",
   "metadata": {},
   "source": [
    "1-The goal of the dataset\n",
    "\n",
    "The goal of the dataset is to develop a personalized fitness recommendation system to classify individuals into suitable fitness types for tailored workout plans and improved training efficiency.\n",
    "\n",
    "2-The source of the dataset\n",
    "\n",
    "https://data.mendeley.com/datasets/zw8mtbm5b9/1\n",
    "\n",
    "3-General Information\n",
    "\n",
    "Structure of the Dataset -Number of Observations: The dataset contains 3,695 records, meaning it includes data for 3,695 different individuals.\n",
    "\n",
    "-Number of Variables: There are 8 relevant variables , but we used weight , height , and age for predicting the fitness type.\n",
    "Variables and Their Types\n",
    "\n",
    "-Sex: Represents the gender of the individual. Possible values: Male, Female\n",
    "\n",
    "-Age(Integer):Represents the age of the individual.\n",
    "\n",
    "-Height(Float):Represents the height of the individual in meters.\n",
    "\n",
    "-Weight(Float):Represents the weight of the individual in kilograms.\n",
    "\n",
    "-BMI(Float):Calculated as Weight (kg) / Height² (m²). It indicates whether an individual is underweight, normal weight, overweight, or obese.\n",
    "\n",
    "-Level(Categorical):Represents the BMI classification of the individual. Possible values: Underweight ,Normal ,Overweight, Obese.\n",
    "\n",
    "-Fitness Goal(Categorical):Describes the individual's primary fitness objective. Possible values: Weight Gain ,Weight Loss\n",
    "\n",
    "-Fitness Type(Label):This is the output variable that the model aims to predict. Represents the recommended fitness category based on an individual's characteristics and goals.\n",
    "\n",
    "Possible values: Muscular Fitness ,Cardio Fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4- Summary of the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('Dataset/gym_recommendation.csv')\n",
    "\n",
    "\n",
    "# 1 Displaying the First Few Rows\n",
    "print(\"Displaying the first few rows of the dataset allows us to understand its structure and confirm it has been loaded correctly. This initial check helps identify any unexpected issues, such as incorrect data types or formatting problems, before we proceed with deeper analysis.\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set consistent style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define colors\n",
    "num_color = \"steelblue\"\n",
    "cat_color = \"coral\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set consistent style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define colors\n",
    "num_color = \"steelblue\"\n",
    "cat_color = \"coral\"\n",
    "\n",
    "# 2 Visualizing Numerical Variable Distributions\n",
    "print(\"Visualizing the distributions of numerical variables like Age, Height, Weight, and BMI is crucial. These factors significantly influence fitness classifications and personalized recommendations. By using histograms, we can detect imbalances in the data, such as a predominance of certain age groups or BMI ranges, and identify outliers that could impact model performance.\")\n",
    "numerical_vars = ['Age', 'Height', 'Weight', 'BMI']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(\"Distribution of Numerical Variables\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    row, col = divmod(i, 2)\n",
    "    sns.histplot(df[var], bins=15, color=num_color, kde=True, ax=axes[row, col])\n",
    "    axes[row, col].set_title(var)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# 3 Visualizing Categorical Variable Distributions\n",
    "print(\"Next, we visualize categorical variables like Sex, Fitness Goal, and Fitness Type. Understanding how these categories are distributed is essential for building a personalized recommendation system. Insights into Fitness Goals and Types help ensure the recommendations align with user preferences and needs.\")\n",
    "categorical_vars = ['Sex', 'Fitness Goal', 'Fitness Type']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(\"Distribution of Categorical Variables\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for i, var in enumerate(categorical_vars):\n",
    "    sns.countplot(data=df, x=var, hue=var, legend=False, ax=axes[i])\n",
    "    axes[i].set_title(var)\n",
    "    axes[i].set_xticks(range(len(df[var].unique())))\n",
    "    axes[i].set_xticklabels(df[var].unique(), rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Handling Missing Values\n",
    "print(\"Checking for missing values is vital as they can lead to incorrect recommendations. Confirming that our dataset has no missing values indicates high data quality, allowing us to proceed without the need for imputation techniques.\")\n",
    "print(\"Missing Values Summary:\\n\")\n",
    "print(missing_values.to_frame(name=\"Missing values in each column\"))  # Display as a table-like format\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"\\n No missing values found in the dataset!\")\n",
    "\n",
    "print(\"To further clarify, here is a heatmap of missing values, showing a solid color as there are no missed values\")\n",
    "# Heatmap of missing values\n",
    "plt.figure(figsize=(8, 4))  # Adjust size for better readability\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"Purples\", yticklabels=False, linewidths=0.5, linecolor=\"#5A86AD\")\n",
    "plt.title(\"Missing Values Heatmap\", fontsize=12, fontweight=\"bold\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "numerical_cols = ['Age', 'Height', 'Weight', 'BMI']  \n",
    "\n",
    "# Statistical Summary \n",
    "print(\"\\n\\nStatistical Summary:\")  \n",
    "print(\"The following table provides descriptive statistics (count, mean, std, min, max, percentiles) for Age, Height, Weight, and BMI.\\nUnderstanding the distribution and range of these user characteristics is crucial for personalized fitness recommendations.\\n\")\n",
    "print(df[numerical_cols].describe())\n",
    "\n",
    "# Variance \n",
    "print(\"\\nVariance of numerical variables:\")  \n",
    "print(\"Variance measures the spread of data around the mean.\\n \")\n",
    "print(df[numerical_cols].var())\n",
    "\n",
    "# BMI vs Fitness Goal (Does a higher BMI correlate with more \"Weight Loss\" goals?)\n",
    "# Justification for BMI vs. Fitness Goal Plot:\n",
    "print(\"\\n\\nBMI vs. Fitness Goal:\")\n",
    "print(\"This bar plot visualizes the average BMI for each fitness goal category. It helps explore the relationship\\nbetween BMI and the fitness goal. We can assess if individuals with higher BMIs are more likely to have\\n'Weight Loss' as their fitness goal.\")\n",
    "sns.barplot(data=df, x=\"Fitness Goal\", y=\"BMI\")  \n",
    "plt.title(\"Average BMI for Each Fitness Goal\")\n",
    "plt.show()\n",
    "\n",
    "# Weight & Height VS BMI (Do weight and height strongly influence BMI?)\n",
    "# Justification for Weight & Height vs. BMI Plot:\n",
    "print(\"\\nWeight & Height vs. BMI:\")\n",
    "print(\"This scatter plot visualizes the relationship between weight and height, with BMI represented by color.\\nIt helps understand how these two fundamental measurements influence BMI. We expect to see a strong correlation,\\nas BMI is calculated directly from weight and height.\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='Height', y='Weight', hue='BMI', palette='viridis')\n",
    "plt.title('Weight vs. Height (Colored by BMI)')\n",
    "plt.xlabel('Height (m)')\n",
    "plt.ylabel('Weight (kg)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b215a970",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: Dataset\\gym recommendation1.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Check if files exist\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file1):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file2):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File not found: Dataset\\gym recommendation1.csv"
     ]
    }
   ],
   "source": [
    "#5-Preprocessing techniques\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Define file paths\n",
    "dataset_dir = \"Dataset\"\n",
    "file1 = os.path.join(dataset_dir, \"gym recommendation1.csv\")\n",
    "file2 = os.path.join(dataset_dir, \"gym recommendation_Cleaned.csv\")\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(file1):\n",
    "    raise FileNotFoundError(f\"File not found: {file1}\")\n",
    "if not os.path.exists(file2):\n",
    "    raise FileNotFoundError(f\"File not found: {file2}\")\n",
    "\n",
    "# Load the datasets\n",
    "df_processed = pd.read_csv(file1)  # Original dataset before removing columns & duplicates\n",
    "df_original_deduped = pd.read_csv(file2)  # Dataset after removing duplicates, before normalization\n",
    "\n",
    "# 1. Variable Removal: Drop unnecessary columns\n",
    "#Remove columns like Exercises, Equipment, Diet, Recommendation: For building a recommendation system focusing on Fitness Type, other details might distract the model or are not needed for the primary goal.\n",
    "columns_to_drop = [\"ID\", \"Exercises\", \"Equipment\", \"Diet\", \"Recommendation\"]\n",
    "df_full = df_processed.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "\n",
    "# 2. Duplicate Removal: Ensure duplicates are removed\n",
    "# Remove Duplicate Rows: Duplicate entries can lead to biases in the model, as they may overrepresent certain patterns. Removing duplicates helps ensure the model is trained on diverse and unique data instances, improving generalization.\n",
    "# Identify and remove duplicate rows in the original dataset\n",
    "df_processed_deduped = df_full.drop_duplicates()\n",
    "\n",
    "# 3. Variable Transformation and Encoding\n",
    "#Categorical Encoding: Columns like Sex, Level, Fitness Goal should be encoded into numerical formats suitable for machine learning algorithms. This can be achieved using label encoding or one-hot encoding, depending on the model requirements.\n",
    "#Label Encoding for Fitness Type: This is the target variable, so it should be label encoded for classification purposes.\n",
    "categorical_cols = [\"Sex\", \"Level\", \"Fitness Goal\", \"Hypertension\", \"Diabetes\", \"Fitness Type\"]\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_processed_deduped[col] = le.fit_transform(df_processed_deduped[col])\n",
    "    label_encoders[col] = le  # Store encoder for later use\n",
    "\n",
    "# 4. Normalization/Scaling for numerical features\n",
    "#Scale Numerical Features (Age, Height, Weight, BMI): These features should be scaled to ensure they contribute equally to the model's performance. Use Min-Max scaling to normalize these features between 0 and 1.\n",
    "scaler = MinMaxScaler()\n",
    "numerical_columns = [\"Age\", \"Height\", \"Weight\", \"BMI\"]\n",
    "\n",
    "df_processed_deduped[numerical_columns] = scaler.fit_transform(df_processed_deduped[numerical_columns])\n",
    "\n",
    "# Save the processed datasets\n",
    "df_original_deduped.to_csv(\"gym_recommendation_original_deduped.csv\", index=False)\n",
    "df_processed_deduped.to_csv(\"gym_recommendation_processed_deduped.csv\", index=False)\n",
    "\n",
    "print(\"✅ Preprocessing completed successfully. Files saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c0949",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c052c7",
   "metadata": {},
   "source": [
    "#Phase2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1025fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Cross-Validation Scores: [0.98671648 0.99335548 0.98006645 0.99335548 0.95930233]\n",
      "SVM Cross-Validation Scores: [0.99667912 0.99335548 0.99335548 0.98837209 0.97342193]\n",
      "Decision Tree Average Cross-Validation Accuracy: 0.98\n",
      "SVM Average Cross-Validation Accuracy: 0.99\n",
      "\n",
      "Decision Tree Cross-Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      6201\n",
      "           1       0.98      0.98      0.98      5840\n",
      "\n",
      "    accuracy                           0.98     12041\n",
      "   macro avg       0.98      0.98      0.98     12041\n",
      "weighted avg       0.98      0.98      0.98     12041\n",
      "\n",
      "\n",
      "SVM Cross-Validation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      6201\n",
      "           1       1.00      0.98      0.99      5840\n",
      "\n",
      "    accuracy                           0.99     12041\n",
      "   macro avg       0.99      0.99      0.99     12041\n",
      "weighted avg       0.99      0.99      0.99     12041\n",
      "\n",
      "✅ SVM model saved for future predictions.\n",
      "Predicted Fitness Type: Muscular\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# 1 Load Preprocessed Data\n",
    "file_path = \"gym_recommendation_processed_deduped.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2 Separate Features & Target Variable\n",
    "target_column = \"Fitness Type\"\n",
    "\n",
    "# Features: Age, Height, Weight\n",
    "input_features = [\"Age\", \"Height\", \"Weight\"]\n",
    "X = df[input_features]\n",
    "y = df[target_column]\n",
    "\n",
    "# 3 Split Data into Training (80%) and Testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4 Train Decision Tree Classifier (ID3 Algorithm)\n",
    "dt_model = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# 5 Train Support Vector Machine (SVM - Linear Kernel)\n",
    "svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# 6 Cross-Validation Evaluation\n",
    "dt_cv_scores = cross_val_score(dt_model, X, y, cv=5)\n",
    "svm_cv_scores = cross_val_score(svm_model, X, y, cv=5)\n",
    "\n",
    "print(\"\\nDecision Tree Cross-Validation Scores:\", dt_cv_scores)\n",
    "print(\"SVM Cross-Validation Scores:\", svm_cv_scores)\n",
    "\n",
    "print(f\"Decision Tree Average Cross-Validation Accuracy: {dt_cv_scores.mean():.2f}\")\n",
    "print(f\"SVM Average Cross-Validation Accuracy: {svm_cv_scores.mean():.2f}\")\n",
    "\n",
    "# 7 Generate Cross-Validated Predictions for Classification Reports\n",
    "y_pred_dt_cv = cross_val_predict(dt_model, X, y, cv=5)\n",
    "y_pred_svm_cv = cross_val_predict(svm_model, X, y, cv=5)\n",
    "\n",
    "# 8 Classification Reports for Cross-Validated Predictions\n",
    "print(\"\\nDecision Tree Cross-Validation Classification Report:\\n\", classification_report(y, y_pred_dt_cv))\n",
    "print(\"\\nSVM Cross-Validation Classification Report:\\n\", classification_report(y, y_pred_svm_cv))\n",
    "\n",
    "# 9 Save the Best Model for Future Predictions\n",
    "best_model = dt_model if dt_cv_scores.mean() > svm_cv_scores.mean() else svm_model\n",
    "joblib.dump(best_model, \"best_fitness_model.pkl\")\n",
    "\n",
    "# Print the best model that was saved\n",
    "if best_model == dt_model:\n",
    "    print(\"✅ Decision Tree model saved for future predictions.\")\n",
    "else:\n",
    "    print(\"✅ SVM model saved for future predictions.\")\n",
    "\n",
    "# 10 Function to Predict Fitness Type from User Input\n",
    "def predict_fitness(user_input):\n",
    "    model = joblib.load(\"best_fitness_model.pkl\")\n",
    "    input_df = pd.DataFrame([user_input], columns=input_features)\n",
    "    prediction = model.predict(input_df)[0]\n",
    "    return prediction\n",
    "\n",
    "# 11 Get User Input\n",
    "try:\n",
    "    age = float(input(\"Enter Age: \"))\n",
    "    height = float(input(\"Enter Height: \"))\n",
    "    weight = float(input(\"Enter Weight: \"))\n",
    "\n",
    "    user_input = {\n",
    "        \"Age\": age,\n",
    "        \"Height\": height,\n",
    "        \"Weight\": weight\n",
    "    }\n",
    "\n",
    "    # 12 Predict Fitness Type\n",
    "    predicted_fitness_num = predict_fitness(user_input)\n",
    "    # Map Numerical Prediction to Labels\n",
    "    if predicted_fitness_num == 0:\n",
    "        predicted_fitness_label = \"Cardio\"\n",
    "    else:\n",
    "        predicted_fitness_label = \"Muscular\"\n",
    "\n",
    "    print(\"Predicted Fitness Type:\", predicted_fitness_label)\n",
    "\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter numerical values for Age, Height, and Weight.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9c5df",
   "metadata": {},
   "source": [
    "#discussing the result:\n",
    "\n",
    "The performance comparison between the Decision Tree (ID3 algorithm) and Support Vector Machine (SVM - Linear Kernel) was conducted using cross-validation accuracy and classification reports. The Decision Tree model achieved an average cross-validation accuracy of 0.98, with some variation across different folds, while the SVM model had a slightly higher accuracy of 0.99, demonstrating better consistency and generalization.\n",
    "\n",
    "From the classification reports, both models performed well, but SVM showed slightly better results. The Decision Tree had an F1-score of 0.98 for both classes, while the SVM achieved 0.99, indicating fewer misclassifications. Additionally, the recall for class 0 (Cardio) in the SVM model was 1.00, meaning all cardio cases were correctly classified, while class 1 (Muscular) had a recall of 0.98, suggesting minor misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e5887",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d0889",
   "metadata": {},
   "source": [
    "#which model is best ?\n",
    "\n",
    "Based on the cross-validation results, the SVM model was selected due to its slightly higher accuracy and greater consistency, indicating better generalization to unseen data. The SVM consistently achieved an average cross-validation accuracy of 0.99, slightly outperforming the Decision Tree's 0.98. Moreover, the SVM's classification report revealed exceptional recall for class 0 and precision for class 1, both reaching 1.00. This suggests that the SVM model is better at capturing the underlying patterns in the data and demonstrates greater consistency across different data partitions. Consequently, it is deemed the more reliable choice for making future fitness type predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d9e1d",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f506aa",
   "metadata": {},
   "source": [
    "#why SVM and decision tree?\n",
    "\n",
    "We chose the Support Vector Machine (SVM) algorithm for our supervised learning task of predicting fitness type using weight, height, and age as input features. SVM is well-suited for classification problems where the goal is to find the best boundary that separates different classes. It works effectively in high-dimensional spaces and can handle both linear and non-linear relationships through the use of kernel functions. By maximizing the margin between classes, SVM often provides good generalization to new data, which helps improve the accuracy and robustness of our fitness type predictions.\n",
    "\n",
    "On the other hand, we also used the Decision Tree algorithm because of its simplicity and interpretability. Decision Trees make decisions by splitting the data into subsets based on feature values, creating an easy-to-follow, tree-like model. This makes it possible to visualize the decision-making process and understand which features, such as weight, height, or age, play the most significant role in determining the fitness type. Additionally, Decision Trees handle non-linear data well and are less affected by outliers, providing a straightforward approach to model training and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498badce",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f8c02",
   "metadata": {},
   "source": [
    "#why high accuracy? \n",
    "\n",
    "The high accuracy achieved in our model is likely due to the strong relationship between the selected features (Age, Height, and Weight) and the target variable (Fitness Type), making it easier for the model to classify data correctly. If the dataset has well-separated classes, minimal noise, or redundant samples, the model can quickly learn patterns, leading to high accuracy. However, the Decision Tree classifier is more prone to overfitting, meaning it memorizes training data rather than generalizing well, which was confirmed through cross-validation results showing high variance. On the other hand, SVM demonstrated better generalization, making it a more reliable choice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
